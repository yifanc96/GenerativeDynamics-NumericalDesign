{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d678cd0-5900-44ff-a914-a7d98af31360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision import transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import math\n",
    "import argparse\n",
    "import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125d66e4-bb6c-4ac2-815d-13c134adbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ data loading ################\n",
    "\n",
    "def itr_merge(itrs):\n",
    "    for itr in itrs:\n",
    "        for v in enumerate(itr):\n",
    "            yield v\n",
    "\n",
    "def get_many_forecasting_dataloader(list_loc, time_lag, lo_size, hi_size, batch_size, train_test_split, subsampling_ratio = None):\n",
    "    \"\"\"\n",
    "    get forecasting dataloader for 2D Stochastic NSE\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_loc: list of locations of data files, of dim num_trajectory*num_snapshots*res*res\n",
    "    time_lag: creat forecasting data with time_lag \n",
    "        (i.e. x_t = data[:,:-time_lag,...] and x_{t+tau}=data[:,time_lag:,...])\n",
    "    lo_size, hi_size: resolutions of x_t and x_{t+tau}\n",
    "    batch_size: batch size\n",
    "    train_test_split: a ratio representing the splitting of training/testing data\n",
    "    subsampling_ratio: used for subsampling a small portion of data, for convenient small scale experiments\n",
    "    \"\"\"\n",
    "\n",
    "    avg_pixel_norm = 3.0679163932800293 # avg data norm computed a priori\n",
    "    \n",
    "    print(f'[prepare dataset] time lag {time_lag}')\n",
    "    \n",
    "    list_len = len(list_loc)\n",
    "    list_train_loaders = []\n",
    "    list_test_loaders = []\n",
    "    \n",
    "    for i in range(list_len):\n",
    "        print(f'---- [data set loc {i}] {list_loc[i]}')\n",
    "        data_raw,time_raw = torch.load(list_loc[i])\n",
    "        Ntj, Nts, Nx, Ny = data_raw.size() \n",
    "        tmp = torch.norm(data_raw,dim=(2,3),p='fro').mean() / np.sqrt(Nx*Ny)\n",
    "        print(f'---- [dataset] average pixel norm of data set {tmp.item()}')\n",
    "        data_raw = data_raw/avg_pixel_norm\n",
    "        \n",
    "        if time_lag > 0:\n",
    "            data_pre = data_raw[:,:-time_lag,...]\n",
    "            data_post = data_raw[:,time_lag:,...]\n",
    "        else:\n",
    "            data_pre = data_raw\n",
    "            data_post = data_raw\n",
    "\n",
    "        print(f'---- [processing] low resolution {lo_size}, high resolution {hi_size}')\n",
    "        hi = torch.nn.functional.interpolate(data_post, size=(hi_size,hi_size),mode='bilinear').reshape([-1,hi_size,hi_size])\n",
    "        lo = torch.nn.functional.interpolate(data_pre, size=(lo_size,lo_size),mode='bilinear')\n",
    "        m = nn.Upsample(scale_factor=int(hi_size/lo_size), mode='nearest')\n",
    "        lo = m(lo).reshape([-1,hi_size,hi_size])\n",
    "        hi = hi[:,None,:,:] # make the data N C H W\n",
    "        lo = lo[:,None,:,:] \n",
    "        \n",
    "        if subsampling_ratio:\n",
    "            hi = hi[:int(subsampling_ratio*hi.size()[0]),...]\n",
    "            lo = lo[:int(subsampling_ratio*lo.size()[0]),...]\n",
    "        \n",
    "        num_train = int(lo.size()[0]*train_test_split)\n",
    "        print(f'---- [processing] train_test_split {train_test_split}, num of training {num_train}, testing {lo.size()[0]-num_train}')\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(lo[:num_train,...],hi[:num_train,...]), batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(lo[num_train:,...],hi[num_train:,...]), batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        list_train_loaders.append(train_loader)\n",
    "        list_test_loaders.append(test_loader)\n",
    "        del data_raw\n",
    "    \n",
    "    new_avg_pixel_norm = 1.0\n",
    "    \n",
    "    return list_train_loaders, list_test_loaders, avg_pixel_norm, new_avg_pixel_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ae3a08-22bd-460e-8295-414b69770485",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ network ################\n",
    "\n",
    "from unet import Unet\n",
    "class Velocity(nn.Module):\n",
    "    \"\"\" \n",
    "    This is a wrapper around any architecture\n",
    "    The warpper handles the additional conditioning input by appending conditioning input as a channel\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(Velocity, self).__init__()\n",
    "        self.config = config\n",
    "        self._arch = Unet(\n",
    "            num_classes = config.num_classes,\n",
    "            in_channels = config.C + config.cond_channels,\n",
    "            out_channels= config.C,\n",
    "            dim = config.unet_channels,\n",
    "            dim_mults = config.unet_dim_mults,\n",
    "            resnet_block_groups = config.unet_resnet_block_groups,\n",
    "            learned_sinusoidal_cond = config.unet_learned_sinusoidal_cond,\n",
    "            random_fourier_features = config.unet_random_fourier_features,\n",
    "            learned_sinusoidal_dim = config.unet_learned_sinusoidal_dim,\n",
    "            attn_dim_head = config.unet_attn_dim_head,\n",
    "            attn_heads = config.unet_attn_heads,\n",
    "            use_classes = config.unet_use_classes,\n",
    "        )\n",
    "        num_params = np.sum([int(np.prod(p.shape)) for p in self._arch.parameters()])\n",
    "        print(\"[Network] Num params in main arch for velocity is\", f\"{num_params:,}\")\n",
    "\n",
    "    def forward(self, zt, t, y, cond=None):\n",
    "        inputs = zt\n",
    "        if cond is not None:\n",
    "            \"\"\"appending conditioning input as a channel\"\"\" \n",
    "            inputs = torch.cat([inputs, cond], dim=1)\n",
    "        if not self.config.unet_use_classes:\n",
    "            y = None\n",
    "        out = self._arch(inputs, t, y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe50ec2-1639-4306-8784-c3486e7b642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ interpolants and sampler ################\n",
    "\n",
    "class Interpolants:\n",
    "    \n",
    "    \"\"\" \n",
    "    Definition of interpolants\n",
    "    I_t = alpha x_0 + beta x_1 (x_0 is Gaussian base)\n",
    "    R_t = alpha_dot x_0 + beta_dot x_1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(Interpolants, self).__init__()\n",
    "        self.config = config\n",
    "        print(f'[Interpolants] noise strength is {config.noise_strength}, multiplied with avg_pixel_norm {config.avg_pixel_norm}')\n",
    "        \n",
    "    def alpha(self, t):\n",
    "        return 1-t\n",
    "\n",
    "    def alpha_dot(self, t):\n",
    "        return -1.0 * torch.ones_like(t)\n",
    "\n",
    "    def beta(self, t):\n",
    "        return t\n",
    "\n",
    "    def beta_dot(self, t):\n",
    "        return 1.0 * torch.ones_like(t)\n",
    "    \n",
    "    def wide(self, x):\n",
    "        return x[:, None, None, None]\n",
    "\n",
    "    def It(self, D):\n",
    "        \"\"\"\n",
    "        D is a dictionary containing \n",
    "        x0 = z0, \n",
    "        x1 = z1, \n",
    "        zt = I_t = alpha x_0 + beta x_1\n",
    "        \"\"\"\n",
    "        z0 = D['z0']\n",
    "        z1 = D['z1']\n",
    "        t = D['t']\n",
    "\n",
    "        aterm = self.wide(self.alpha(t)) * z0\n",
    "        bterm = self.wide(self.beta(t)) * z1\n",
    "\n",
    "        D['zt'] = aterm + bterm\n",
    "        return D\n",
    "\n",
    "    def Rt(self, D):\n",
    "        \"\"\"\n",
    "        D is a dictionary containing \n",
    "        x0 = z0, \n",
    "        x1 = z1, \n",
    "        R_t = alpha_dot x_0 + beta_dot x_1\n",
    "        \"\"\"\n",
    "        z0 = D['z0']\n",
    "        z1 = D['z1']\n",
    "        t = D['t']\n",
    "\n",
    "        adot = self.wide(self.alpha_dot(t))\n",
    "        bdot = self.wide(self.beta_dot(t))\n",
    "        return (adot * z0) + (bdot * z1)\n",
    "\n",
    "    \n",
    "\n",
    "class Sampler:\n",
    "    \"\"\"\n",
    "    sampler \n",
    "    self.interpolant: get information from the defined interpolants\n",
    "    self.logger: information for uploading results to wandb, used in self.log_wandb_figure\n",
    "    self.EM: EM for sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = Loggers(config)\n",
    "        self.interpolant = Interpolants(config)\n",
    "        return\n",
    "    \n",
    "    def wide(self, x):\n",
    "        return x[:, None, None, None]\n",
    "\n",
    "    def EM(self, D, model, steps = 200):  # here it is EM, for improved performance, should use odeint\n",
    "        print('[Sampler] Use EM samplers')\n",
    "        init_condition = D['z0']\n",
    "        tgrid = torch.linspace(self.config.t_min_sample, self.config.t_max_sample, steps).type_as(init_condition)\n",
    "        dt = tgrid[1] - tgrid[0]\n",
    "        zt = D['z0']\n",
    "        y = D['y']\n",
    "        cond = D['cond']\n",
    "        ones = torch.ones(zt.shape[0]).type_as(zt)\n",
    "        for tscalar in tgrid:\n",
    "            t_arr = tscalar * ones\n",
    "            f = model(zt, t_arr, y, cond = cond) # note we condiition on init cond\n",
    "            zt_mean = zt + f * dt\n",
    "            zt = zt_mean\n",
    "        return zt_mean\n",
    "\n",
    "    def odeint(self, D, model):\n",
    "        print('[Sampler] Use odeint samplers')\n",
    "        return self.pflow(\n",
    "            b = model,\n",
    "            z0 = D['z0'],\n",
    "            y = D['y'],\n",
    "            cond = D['cond'],\n",
    "            T_min = self.config.t_min_sample,\n",
    "            T_max = self.config.t_max_sample,\n",
    "            steps = 3, # this is just a placeholder, dopri5 is adaptive and doesn't look at steps arg.\n",
    "            method = 'dopri5',\n",
    "            return_last = True,\n",
    "        )\n",
    "    \n",
    "    # for plotting samples\n",
    "    def to_grid(self, x, normalize):\n",
    "        nrow = 1\n",
    "        if normalize:\n",
    "            kwargs = {'normalize' : True, 'value_range' : (-1, 1)}\n",
    "        else:\n",
    "            kwargs = {}\n",
    "        return make_grid(x, nrow = nrow, **kwargs)\n",
    "\n",
    "    # for logging sampled images to wandb\n",
    "    def log_wandb_figure(self, sample, D, global_step):\n",
    "        \"\"\"\n",
    "        plot conditioning input, x0, sampled x1, truth x1\n",
    "        here D includes conditioning input, x0, truth x1\n",
    "        and sample includes sampled x1\n",
    "        finally, upload the figures to wandb\n",
    "        \"\"\"\n",
    "        home = self.config.home\n",
    "        if not os.path.exists(home + \"images\"):\n",
    "            os.makedirs(home + \"images\")\n",
    "        if self.config.use_wandb:\n",
    "            def get_tensor_from_figures(tensor_w):\n",
    "                plt.ioff()\n",
    "                x = tensor_w.cpu()\n",
    "                num = x.size()[0]\n",
    "                for i in range(num):\n",
    "                    WIDTH_SIZE = 3\n",
    "                    HEIGHT_SIZE = 3\n",
    "                    plt.ioff()\n",
    "                    fig = plt.figure(figsize=(WIDTH_SIZE,HEIGHT_SIZE))\n",
    "                    plt.imshow(x[i,0,...], cmap=sns.cm.icefire, vmin=-2, vmax=2.)\n",
    "                    plt.axis('off')\n",
    "                    plt.savefig(home + f'images/tmp{i}_{self.logger.log_base}.jpg', bbox_inches='tight')\n",
    "                    plt.close(\"all\") \n",
    "                    \n",
    "                tensor_img = T.ToTensor()(Image.open(home + f'images/tmp1_{self.logger.log_base}.jpg'))\n",
    "                C, H, W = tensor_img.size()\n",
    "                final_tensor = torch.zeros((num,C,H,W))\n",
    "                for i in range(num):\n",
    "                    tensor_img = T.ToTensor()(Image.open(home + f'images/tmp{i}_{self.logger.log_base}.jpg'))\n",
    "                    final_tensor[i,...] = tensor_img\n",
    "                return final_tensor\n",
    "            \n",
    "            normalize = False\n",
    "            \n",
    "            num_train = config.num_reference_batch_train\n",
    "            \n",
    "            sample = get_tensor_from_figures(sample)\n",
    "            sample_train = self.to_grid(sample[:num_train,...], normalize = normalize)\n",
    "            sample_test = self.to_grid(sample[num_train:,...], normalize = normalize)\n",
    "            \n",
    "            z0 = get_tensor_from_figures(D['z0'])\n",
    "            z0_train = self.to_grid(z0[:num_train,...], normalize = normalize)\n",
    "            z0_test = self.to_grid(z0[num_train:,...], normalize = normalize)\n",
    "            \n",
    "            z1 = get_tensor_from_figures(D['z1'])\n",
    "            z1_train = self.to_grid(z1[:num_train,...], normalize = normalize)\n",
    "            z1_test = self.to_grid(z1[num_train:,...], normalize = normalize)\n",
    "            \n",
    "            # zcond = get_tensor_from_figures(D['cond'])\n",
    "            # zcond_train = self.to_grid(zcond[:num_train,...], normalize = normalize)\n",
    "            # zcond_test = self.to_grid(zcond[num_train:,...], normalize = normalize)\n",
    "            \n",
    "            # both_train = torch.cat([zcond_train, z0_train, sample_train, z1_train], dim=-1)\n",
    "            # both_test = torch.cat([zcond_test, z0_test, sample_test, z1_test], dim=-1)\n",
    "\n",
    "            both_train = torch.cat([z0_train, sample_train, z1_train], dim=-1)\n",
    "            both_test = torch.cat([z0_test, sample_test, z1_test], dim=-1)\n",
    "            \n",
    "            wandb.log({'training-x0_sampledx1_truthx1': wandb.Image(both_train)}, step = global_step)\n",
    "            wandb.log({'testing-x0_sampledx1_truthx1': wandb.Image(both_test)}, step = global_step)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, D, model, global_step, wand_log = True):\n",
    "        model.eval()\n",
    "        if self.config.model_type == 'sde':\n",
    "            zT = self.EM(D, model)\n",
    "        else:\n",
    "            assert False\n",
    "        if self.config.use_wandb and wand_log:\n",
    "            self.log_wandb_figure(zT, D, global_step)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd80333d-17d1-4bda-ac8a-e895c628c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ trainer ################\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer\n",
    "    self.prepare_dataset: create dataloaders using provided locations of data files\n",
    "    self.time_dist: used for sampling time during training\n",
    "    self.set_reference_batch: randomly picking a small training set and testing set, and test on them on the fly\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.prepare_dataset(subsampling_ratio = config.data_subsampling_ratio)\n",
    "        self.interpolant = Interpolants(self.config)\n",
    "        self.model = Velocity(self.config)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = self.get_optimizer(config)\n",
    "        self.sampler = Sampler(config)\n",
    "        self.time_dist = torch.distributions.Uniform(low=self.config.t_min_train, high=self.config.t_max_train)\n",
    "        self.current_epoch = 0\n",
    "        self.global_step = 0\n",
    "        self.set_reference_batch(num_train = config.num_reference_batch_train, num_test = config.num_reference_batch_test) # for efficient test\n",
    "        self.EMsteps = config.EMsteps\n",
    "        self.home = config.home\n",
    "        print(f'[save_loc] will save all checkpoints and results to location to {self.home}')\n",
    "        \n",
    "    def prepare_dataset(self, subsampling_ratio = None):\n",
    "        self.list_train_loaders, self.list_test_loaders, self.original_avg_pixel_norm, self.avg_pixel_norm = get_many_forecasting_dataloader(self.config.list_data_loc, self.config.time_lag, self.config.lo_size, self.config.hi_size, self.config.batch_size, self.config.train_test_split, subsampling_ratio)\n",
    "        self.config.avg_pixel_norm = self.avg_pixel_norm\n",
    "        self.config.original_avg_pixel_norm = self.original_avg_pixel_norm\n",
    "        \n",
    "    def set_reference_batch(self, num_train = 10, num_test = 10):\n",
    "        xlo_train,xhi_train = next(iter(self.list_train_loaders[0]))\n",
    "        xlo_test,xhi_test = next(iter(self.list_test_loaders[0]))\n",
    "        self.ref_xlo = torch.cat((xlo_train[0:num_train,...],xlo_test[0:num_train,...]),0)\n",
    "        self.ref_xhi = torch.cat((xhi_train[0:num_test,...],xhi_test[0:num_test,...]),0)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def prepare_batch(self, batch, time = 'unif', use_reference_batch = False):\n",
    "        \"\"\"\n",
    "        D: a dictionary of x0, x1, and t, for interpolants\n",
    "        here x0 = Gaussian noise\n",
    "             x1 = z1\n",
    "             cond = z0\n",
    "             t = uniform samples from [0,1]\n",
    "        \"\"\"\n",
    "        \n",
    "        if use_reference_batch:\n",
    "            xlo, xhi = self.ref_xlo, self.ref_xhi\n",
    "        else:\n",
    "            xlo, xhi = batch\n",
    "            \n",
    "        y = torch.zeros(xlo.size()[0]) # dummy variable; we do not use labels\n",
    "        xlo, xhi, y = xlo.to(self.device), xhi.to(self.device), y.to(self.device)\n",
    "\n",
    "        tmp = torch.randn_like(xhi)\n",
    "        D = {'z0': tmp, 'z1': xhi, 'cond': xlo, 'y': y}\n",
    "        \n",
    "        if time == 'unif':\n",
    "            D['t'] = self.time_dist.sample(sample_shape = (xhi.shape[0],)).squeeze().type_as(D['z1'])\n",
    "        else:\n",
    "            assert False\n",
    "        D = self.interpolant.It(D)\n",
    "        return D\n",
    "    \n",
    "    def get_optimizer(self, config):\n",
    "        if config.optimizer == \"AdamW\":\n",
    "            print(f'[Optimizer] set up optimizer as {config.optimizer}')\n",
    "            self.lr = self.config.base_lr\n",
    "            return torch.optim.AdamW(self.model.parameters(), lr=self.config.base_lr)\n",
    "    \n",
    "    def target_function(self, D):\n",
    "        if self.config.model_type == 'sde':\n",
    "            target = self.interpolant.Rt(D)  \n",
    "        else:\n",
    "            assert False\n",
    "        return target\n",
    "    \n",
    "    def loss_function(self, D):\n",
    "        assert self.model.training\n",
    "        model_out = self.model(D['zt'], D['t'], D['y'], cond = D['cond'])\n",
    "        target = self.target_function(D)\n",
    "        loss = (model_out - target).pow(2).sum(-1).sum(-1).sum(-1) # using full squared loss here\n",
    "        return loss.mean()\n",
    "    \n",
    "    def clip_grad_norm(self, model, max_grad_norm = 1e+4):\n",
    "        return torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = max_grad_norm, norm_type = 2.0, error_if_nonfinite = False)\n",
    "\n",
    "    def optimizer_one_step(self, max_grad_norm = 1e+4):\n",
    "        self.clip_grad_norm(self.model, max_grad_norm = max_grad_norm)\n",
    "        if self.global_step % self.config.print_loss_every == 0:\n",
    "            grads = [ param.grad.detach().flatten() for param in self.model.parameters() if param.grad is not None]\n",
    "            norm = torch.cat(grads).norm()\n",
    "            print(f\"[Training] Grad step {self.global_step}. Grad norm:{norm}\")\n",
    "            if self.config.use_wandb:\n",
    "                wandb.log({\"Gradnorm\": norm}, step = self.global_step)\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        self.global_step += 1\n",
    "    \n",
    "    def adjust_learning_rate(self, optimizer):\n",
    "        lr = self.lr\n",
    "        if self.config.cosine_scheduler:\n",
    "            scale = self.global_step / self.config.max_steps\n",
    "            lr *= 0.5 * (1. + math.cos(math.pi * scale))\n",
    "            print(f'[Cosine scheduler] lr is now {lr}')\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def fit(self,):\n",
    "        time_begin = time()\n",
    "        print(\"[Training] starting training\")\n",
    "        self.test_model(first_batch_only = True)\n",
    "        \n",
    "        while self.global_step < self.config.max_steps:\n",
    "            print(f\"[Training] starting epoch {self.current_epoch}\")\n",
    "            \n",
    "            for batch_idx, batch in itr_merge(self.list_train_loaders):\n",
    "   \n",
    "                if self.global_step >= self.config.max_steps:\n",
    "                    break\n",
    "\n",
    "                self.model.train()\n",
    "                loss = self.loss_function(D = self.prepare_batch(batch, use_reference_batch = False))\n",
    "                loss.backward()\n",
    "                self.optimizer_one_step()     \n",
    "                if self.global_step % self.config.sample_every == 0:\n",
    "                    # for monitoring sampling, sampling on reference batch and uploading the results on the fly\n",
    "                    D = self.prepare_batch(batch = None, use_reference_batch = True)\n",
    "                    self.sampler.sample(D, self.model, self.global_step)\n",
    "\n",
    "                if self.global_step % self.config.print_loss_every == 0:\n",
    "                    total_mins = (time() - time_begin) / 60\n",
    "                    print(f\"[Training] Grad step {self.global_step}. Loss:{loss.item()}, finished in {total_mins:.2f} minutes\")\n",
    "                    if self.config.use_wandb:\n",
    "                        wandb.log({\"loss\": loss.item()}, step=self.global_step)\n",
    "                \n",
    "                if self.global_step % self.config.save_model_every == 0:\n",
    "                    self.save_model_checkpoint()\n",
    "                if self.global_step % self.config.test_every == 0:\n",
    "                    self.test_model(first_batch_only = True)\n",
    "                    \n",
    "            self.current_epoch += 1\n",
    "            self.adjust_learning_rate(self.optimizer)\n",
    "    \n",
    "    #### below are testing functions, during the training processes or after training\n",
    "    def save_model_checkpoint(self):\n",
    "        \n",
    "        if not os.path.exists(self.home + f\"checkpoint/{logger.verbose_log_name}\"):\n",
    "            os.makedirs(self.home + f\"checkpoint/{logger.verbose_log_name}\")\n",
    "        save_path = self.home + f\"checkpoint/{logger.verbose_log_name}/model_step{self.global_step}.pth\"\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        print(f'[Saving models] saving models to {save_path}')\n",
    "\n",
    "\n",
    "    def sample_results(self, first_batch_only = True, which = 'test', EMsteps = 200):\n",
    "\n",
    "        assert which in ['train', 'test']\n",
    "        if which == 'test':\n",
    "\n",
    "            loader_list = self.list_test_loaders\n",
    "            #############################\n",
    "            print(f'[Test] sampling on test data')\n",
    "            tot_num = 0\n",
    "            for batch_idx, batch in itr_merge(loader_list):\n",
    "                num = batch[0].shape[0]\n",
    "                tot_num = tot_num + num\n",
    "                \n",
    "            if first_batch_only:\n",
    "                tot_num = self.config.batch_size\n",
    "            print(f'[Test] in total {tot_num} test data, first batch only = {first_batch_only}')\n",
    "\n",
    "        else:\n",
    "            \n",
    "            loader_list = self.list_train_loaders\n",
    "            print(f'[Test] sampling on training data, first batch only = {first_batch_only}')\n",
    "            tot_num = self.config.batch_size\n",
    "\n",
    "\n",
    "        lo_size = config.lo_size\n",
    "        hi_size = config.hi_size\n",
    "        test_truth = torch.zeros(tot_num,1,hi_size,hi_size)\n",
    "        test_input = torch.zeros_like(test_truth)\n",
    "        test_result = torch.zeros_like(test_truth)\n",
    "\n",
    "        self.model.eval()\n",
    "        time_begin = time()\n",
    "        cur_idx = 0\n",
    "\n",
    "        for batch_idx, batch in itr_merge(loader_list):\n",
    "            if first_batch_only and batch_idx > 0: break\n",
    "            with torch.no_grad():\n",
    "                num = batch[0].shape[0]\n",
    "                D = self.prepare_batch(batch, use_reference_batch = False)\n",
    "                test_input[cur_idx:cur_idx+num,...] = batch[0]\n",
    "                test_truth[cur_idx:cur_idx+num,...] = batch[1]\n",
    "                test_result[cur_idx:cur_idx+num,...] = self.sampler.EM(D, self.model, steps = EMsteps)\n",
    "                total_mins = (time() - time_begin) / 60\n",
    "                print(f'batch index {batch_idx}, finished in {total_mins:.2f} minutes')\n",
    "                cur_idx = cur_idx + num\n",
    "\n",
    "        inputs = test_input[:cur_idx,...]\n",
    "        truth = test_truth[:cur_idx,...]\n",
    "        results =  test_result[:cur_idx,...]\n",
    "        results = torch.cat([inputs, truth, results], dim = 1) * self.original_avg_pixel_norm\n",
    "        return results\n",
    "    \n",
    "    def plot_spectra(self, results, which = 'test'):\n",
    "    \n",
    "        assert which in ['train', 'test']\n",
    "\n",
    "        # energy spectrum\n",
    "        if not os.path.exists(self.home + \"images\"):\n",
    "            os.makedirs(self.home + \"images\")\n",
    "        \n",
    "        from energy_spectrum_plot import plot_avg_spectra_compare_Forecasting_torch as plot_spectra\n",
    "\n",
    "        spectrum_save_name = self.home + f\"images/{logger.verbose_log_name}_spectrum_test_on_{which}.jpg\"\n",
    "        plot_spectra(results[:,1,...], results[:,2,...], save_name = spectrum_save_name)\n",
    "        print(f\"spectrum plot saved to {spectrum_save_name}\")\n",
    "        \n",
    "        tensor_img = T.ToTensor()(Image.open(spectrum_save_name))\n",
    "\n",
    "        f = lambda x: wandb.Image(x[None,...])\n",
    "        if config.use_wandb:\n",
    "            wandb.log({f'energy spectrum (test on {which} data)': f(tensor_img)}, step = self.global_step) \n",
    "    \n",
    "    def compute_norm(self, results, which = 'test'):\n",
    "        truth = results[:,1,...]\n",
    "        forecast = results[:,2,...]\n",
    "        truth_norm = torch.norm(truth,dim=(1,2),p='fro').mean() / self.config.hi_size\n",
    "        forecast_norm = torch.norm(forecast,dim=(1,2),p='fro').mean() / self.config.hi_size\n",
    "        relerr = abs(truth_norm-forecast_norm)/truth_norm\n",
    "        print(f\"[testing norms] on {which} data, truth norm is {truth_norm}, forecast norm is {forecast_norm}, relative err {relerr}\")\n",
    "        if self.config.use_wandb:\n",
    "            wandb.log({f\"norm err (on {which} data)\":  relerr}, step = self.global_step)\n",
    "    \n",
    "    def test_model(self, first_batch_only = True):\n",
    "\n",
    "        train_results = self.sample_results(first_batch_only = first_batch_only, which = 'train', EMsteps = self.EMsteps)\n",
    "        test_results = self.sample_results(first_batch_only = first_batch_only, which = 'test', EMsteps = self.EMsteps)\n",
    "\n",
    "        # norm test\n",
    "        self.compute_norm(train_results, which = 'train')\n",
    "        self.compute_norm(test_results, which = 'test')\n",
    "        \n",
    "        # spectrum test\n",
    "        self.plot_spectra(train_results, which = 'train')\n",
    "        self.plot_spectra(test_results, which = 'test')       \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "635d63c5-a603-4d88-8eac-0c5c2908a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ logger ################\n",
    "\n",
    "class Loggers:\n",
    "    \"\"\"\n",
    "    self.log_base: date string for naming of logging files\n",
    "    self.log_name: detailed information of the experiment, used for naming of logging files\n",
    "    self.verbose_log_name: more verbose version for naming\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        date = str(datetime.datetime.now())\n",
    "        self.log_base = date[date.find(\"-\"):date.rfind(\".\")].replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"_\")\n",
    "        self.log_name = 'lag' + str(config.time_lag) + 'noise' + str(config.noise_strength) + 'lo' + str(config.lo_size) + 'hi' + str(config.hi_size) + '_' + self.log_base\n",
    "        self.verbose_log_name = 'GaussODE_numdata'+ str(config.num_dataset) + 'lag' + str(config.time_lag) + 'noise' + str(config.noise_strength) + 'lo' + str(config.lo_size) + 'hi' + str(config.hi_size) + 'sz' + str(config.base_lr).replace(\".\",\"\") + 'max' + str(config.max_steps) + '_' + self.log_base\n",
    "        \n",
    "    def is_type_for_logging(self, x):\n",
    "        if isinstance(x, int):\n",
    "            return True\n",
    "        elif isinstance(x, float):\n",
    "            return True\n",
    "        elif isinstance(x, bool):\n",
    "            return True\n",
    "        elif isinstance(x, str):\n",
    "            return True\n",
    "        elif isinstance(x, list):\n",
    "            return True\n",
    "        elif isinstance(x, set):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def setup_wandb(self, config):\n",
    "        if config.use_wandb:\n",
    "            config.wandb_run = wandb.init(\n",
    "                    project=config.wandb_project,\n",
    "                    entity=config.wandb_entity,\n",
    "                    resume=None,\n",
    "                    id    =None,\n",
    "                    name = self.verbose_log_name,\n",
    "            )\n",
    "            wandb.run.log_code(\".\")\n",
    "\n",
    "            for key in vars(config):\n",
    "                item = getattr(config, key)\n",
    "                if self.is_type_for_logging(item):\n",
    "                    setattr(wandb.config, key, item)\n",
    "                    print(f'[Config] {key}: {item}')\n",
    "            print(\"[wandb] finished wandb setup\")\n",
    "        else:\n",
    "            print(\"[wandb] not using wandb setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1d9f0f-fae7-4fa0-9c90-0daa9e3a7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ config ################\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self,list_data_loc, home = \"./\"):\n",
    "        \n",
    "        # use wandb for logging\n",
    "        self.use_wandb = False\n",
    "        self.wandb_project = 'interpolants_forecasting_new'\n",
    "        self.wandb_entity = 'yifanc96'\n",
    "\n",
    "        self.home = home # for storing checkpoints\n",
    "        \n",
    "        # data\n",
    "        self.list_data_loc = list_data_loc\n",
    "        self.num_dataset = len(list_data_loc)\n",
    "        self.C = 1\n",
    "        self.num_classes = 1\n",
    "        self.lo_size = 128\n",
    "        self.hi_size = 128\n",
    "        self.batch_size = 100\n",
    "        self.num_workers = 4\n",
    "        self.train_test_split = 0.9\n",
    "        self.noise_strength = 0.0\n",
    "        self.data_subsampling_ratio = 1.0  # use a small amount of data, for sanity check of the code\n",
    "        \n",
    "        # training\n",
    "        self.optimizer = 'AdamW'\n",
    "        self.cosine_scheduler = True\n",
    "        self.model_type = 'sde'\n",
    "        self.base_lr = 2*1e-4\n",
    "        self.max_steps = 100\n",
    "        self.t_min_train = 0\n",
    "        self.t_max_train = 1\n",
    "        self.t_min_sample = 0\n",
    "        self.t_max_sample = 1\n",
    "        self.EMsteps = 200\n",
    "        self.print_loss_every = 20 \n",
    "        self.print_gradnorm_every =  20\n",
    "        self.num_reference_batch_train = 10\n",
    "        self.num_reference_batch_test = 10\n",
    "        self.sample_every = 200 # sampling on reference batch every # iterations\n",
    "        self.test_every = 200 # test energy spectrum and norm on reference batch every # iterations\n",
    "        self.save_model_every = 2000 # save model checkpoints every # iterations\n",
    "        \n",
    "        # architecture\n",
    "        self.unet_use_classes = False\n",
    "        self.model_size = 'medium'\n",
    "        if self.model_size == 'small':\n",
    "            self.unet_channels = 8\n",
    "            self.unet_dim_mults = (1, 1, 1, 1)\n",
    "            self.unet_resnet_block_groups = 8\n",
    "            self.unet_learned_sinusoidal_dim = 8\n",
    "            self.unet_attn_dim_head = 8\n",
    "            self.unet_attn_heads = 1\n",
    "            self.unet_learned_sinusoidal_cond = False\n",
    "            self.unet_random_fourier_features = False\n",
    "        \n",
    "        elif self.model_size == 'medium':\n",
    "            self.unet_channels = 32\n",
    "            self.unet_dim_mults = (1, 2, 2, 2)\n",
    "            self.unet_resnet_block_groups = 8\n",
    "            self.unet_learned_sinusoidal_dim = 32\n",
    "            self.unet_attn_dim_head = 32\n",
    "            self.unet_attn_heads = 4\n",
    "            self.unet_learned_sinusoidal_cond = True\n",
    "            self.unet_random_fourier_features = False\n",
    "   \n",
    "        elif self.model_size == 'large':\n",
    "            self.unet_channels = 128\n",
    "            self.unet_dim_mults = (1, 2, 2, 2)\n",
    "            self.unet_resnet_block_groups = 8\n",
    "            self.unet_learned_sinusoidal_dim = 32\n",
    "            self.unet_attn_dim_head = 64\n",
    "            self.unet_attn_heads = 4\n",
    "            self.unet_learned_sinusoidal_cond = True\n",
    "            self.unet_random_fourier_features = False\n",
    "        else:\n",
    "            assert False\n",
    "        self.cond_channels = 0  # dimension of the conditional channel; here conditioned on z_0\n",
    "        # the conditioned term is appended to the input (so the final channel dim = cond_channels + input_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b79d31c-834f-4605-a194-879330b4edd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Argparse] change config data_subsampling_ratio to 1.0\n",
      "[Argparse] change config batch_size to 10\n",
      "[Argparse] change config noise_strength to 0.0\n",
      "[Argparse] change config base_lr to 0.0002\n",
      "[Argparse] change config lo_size to 128\n",
      "[Argparse] change config hi_size to 128\n",
      "[Argparse] change config time_lag to 2\n",
      "[Argparse] change config max_steps to 50000\n",
      "[Argparse] change config sample_every to 1000\n",
      "[Argparse] change config test_every to 1000\n",
      "[Argparse] change config save_model_every to 2000\n",
      "[Argparse] change config num_dataset to 1\n",
      "[Argparse] change config use_wandb to False\n",
      "[wandb] not using wandb setup\n",
      "[prepare dataset] time lag 2\n",
      "---- [data set loc 0] /scratch/yc3400/forecasting/NSEdata/data_file01.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-63034480/ipykernel_2816376/2195593894.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_raw,time_raw = torch.load(list_loc[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- [dataset] average pixel norm of data set 3.072007417678833\n",
      "---- [processing] low resolution 128, high resolution 128\n",
      "---- [processing] train_test_split 0.9, num of training 17640, testing 1960\n",
      "[Interpolants] noise strength is 0.0, multiplied with avg_pixel_norm 1.0\n",
      "NOT USING CLASSES IN UNET\n",
      "[Network] Num params in main arch for velocity is 2,063,857\n",
      "[Optimizer] set up optimizer as AdamW\n",
      "[Interpolants] noise strength is 0.0, multiplied with avg_pixel_norm 1.0\n",
      "[save_loc] will save all checkpoints and results to location to /scratch/yc3400/forecasting/\n",
      "[Training] starting training\n",
      "[Test] sampling on training data, first batch only = True\n",
      "[Sampler] Use EM samplers\n"
     ]
    }
   ],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch framework for stochastic interpolants')\n",
    "    parser.add_argument(\"--data_subsampling_ratio\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=10)\n",
    "    parser.add_argument(\"--noise_strength\", type=float, default=0.0) #for Gaussian base, no noise added\n",
    "    parser.add_argument(\"--base_lr\", type=float, default=2e-4)\n",
    "    parser.add_argument(\"--lo_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--hi_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--time_lag\", type=int, default=2)\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=50000)\n",
    "    parser.add_argument(\"--sample_every\", type=int, default=1000)\n",
    "    parser.add_argument(\"--test_every\", type=int, default=1000)\n",
    "    parser.add_argument(\"--save_model_every\", type=int, default=2000)\n",
    "    parser.add_argument(\"--num_dataset\",type=int,default=1)\n",
    "    parser.add_argument('--use_wandb', type = int, default = 0) # 1 is use_wandb, and 0 is not use_wandb\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(random_seed)\n",
    "args = get_parser()\n",
    "args.use_wandb = bool(args.use_wandb)\n",
    "\n",
    "\n",
    "###### data location\n",
    "list_suffix = [f\"0{i}\" for i in np.arange(1,args.num_dataset+1)]\n",
    "list_data_loc = [f\"/scratch/yc3400/forecasting/NSEdata/data_file\" + i + \".pt\" for i in list_suffix]\n",
    "\n",
    "if args.num_dataset < len(list_data_loc): \n",
    "    list_data_loc = list_data_loc[:args.num_dataset]\n",
    "    args.num_dataset = len(list_data_loc)\n",
    "\n",
    "##### checkpoint and image storage location\n",
    "# home = \"./\"\n",
    "home = \"/scratch/yc3400/forecasting/\" \n",
    "\n",
    "config = Config(list_data_loc, home)\n",
    "for arg in vars(args):\n",
    "    print(f'[Argparse] change config {arg} to {getattr(args, arg)}')\n",
    "    setattr(config, arg, getattr(args, arg))\n",
    "logger = Loggers(config)\n",
    "logger.setup_wandb(config)\n",
    "trainer = Trainer(config)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307ba0d-1f14-42bc-a3f8-820e3381bfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a40160-4b47-47a0-9f36-3ca5b0ca3ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MYKERNEL",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
