{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8253a0-b065-42c9-9625-57bed26c074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader \n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import math\n",
    "import argparse\n",
    "import datetime\n",
    "import math\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9072872-b542-4479-b419-9afb72710a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_twomode_GMM(num_samples, dim, p, r, sigma=1.0, device='cpu', dtype=torch.float32, seed=None):\n",
    "    \"\"\"\n",
    "    Generate samples from a bimodal Gaussian mixture model in d dimensions using PyTorch.\n",
    "    \n",
    "    The mixture is defined as:\n",
    "    ρ*(x) = p*N(x; r, I_d) + (1-p)*N(x; -r, I_d)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    num_samples : int\n",
    "        Number of samples to generate\n",
    "    dim : int\n",
    "        Dimensionality of the data\n",
    "    p : float\n",
    "        Mixing probability for the first component (0 ≤ p ≤ 1)\n",
    "    r : array-like, tensor, or float\n",
    "        Fixed vector for the means. If float, creates vector (r, r, ..., r)\n",
    "        If array/tensor, should have length dim and satisfy |r|_2 = sqrt(d)\n",
    "    sigma : float, default=1.0\n",
    "        Standard deviation for each component (assumes spherical covariance σ²I_d)\n",
    "    device : str or torch.device, default='cpu'\n",
    "        Device to place tensors on ('cpu', 'cuda', etc.)\n",
    "    dtype : torch.dtype, default=torch.float32\n",
    "        Data type for the tensors\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : torch.Tensor of shape (num_samples, dim)\n",
    "        Generated samples from the mixture\n",
    "    labels : torch.Tensor of shape (num_samples,)\n",
    "        Component labels (0 for first component, 1 for second component)\n",
    "    \"\"\"\n",
    "    \n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # Handle r parameter\n",
    "    if torch.is_tensor(r):\n",
    "        r_vec = r.to(device=device, dtype=dtype)\n",
    "    elif hasattr(r, '__len__'):  # array-like\n",
    "        r_vec = torch.tensor(r, device=device, dtype=dtype)\n",
    "    else:  # scalar\n",
    "        r_vec = torch.full((dim,), r, device=device, dtype=dtype)\n",
    "    \n",
    "    if r_vec.numel() != dim:\n",
    "        raise ValueError(f\"r must have length {dim}, got {r_vec.numel()}\")\n",
    "    \n",
    "    # Verify the constraint |r|_2 = sqrt(d) as mentioned in the example\n",
    "    r_norm = torch.norm(r_vec)\n",
    "    expected_norm = torch.sqrt(torch.tensor(dim, dtype=dtype))\n",
    "    if not torch.allclose(r_norm, expected_norm, rtol=1e-6):\n",
    "        print(f\"Warning: |r|_2 = {r_norm.item():.4f}, but sqrt(d) = {expected_norm.item():.4f}\")\n",
    "        print(f\"Consider using r with norm sqrt({dim}) = {expected_norm.item():.4f}\")\n",
    "    \n",
    "    # Generate component assignments using Bernoulli distribution\n",
    "    # torch.bernoulli with p gives 1 with probability p, so we use (1-p) to match the formula\n",
    "    component_probs = torch.full((num_samples,), 1-p, device=device, dtype=dtype)\n",
    "    component_labels = torch.bernoulli(component_probs).long()  # 0 for first, 1 for second\n",
    "    \n",
    "    # Initialize samples tensor\n",
    "    samples = torch.zeros(num_samples, dim, device=device, dtype=dtype)\n",
    "    \n",
    "    # Generate samples for first component: N(x; -r, σ²I_d)\n",
    "    first_mask = (component_labels == 0)\n",
    "    n_first = first_mask.sum().item()\n",
    "    if n_first > 0:\n",
    "        samples[first_mask] = torch.normal(\n",
    "            mean=r_vec.unsqueeze(0).expand(n_first, -1),\n",
    "            std=sigma\n",
    "        )\n",
    "    \n",
    "    # Generate samples for second component: N(x; r, σ²I_d)\n",
    "    second_mask = (component_labels == 1)\n",
    "    n_second = second_mask.sum().item()\n",
    "    if n_second > 0:\n",
    "        samples[second_mask] = torch.normal(\n",
    "            mean=-r_vec.unsqueeze(0).expand(n_second, -1),\n",
    "            std=sigma\n",
    "        )\n",
    "    \n",
    "    return samples, component_labels\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_gmm_pca(samples, labels=None, n_components=2):\n",
    "    \"\"\"\n",
    "    Visualize high-dimensional GMM samples using PCA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : torch.Tensor of shape (num_samples, dim)\n",
    "        High-dimensional samples from GMM\n",
    "    labels : torch.Tensor of shape (num_samples,), optional\n",
    "        Component labels for coloring\n",
    "    n_components : int, default=2\n",
    "        Number of PCA components (2 or 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to numpy\n",
    "    if torch.is_tensor(samples):\n",
    "        samples = samples.detach().cpu().numpy()\n",
    "    if labels is not None and torch.is_tensor(labels):\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    samples_pca = pca.fit_transform(samples)\n",
    "    \n",
    "    # Plot\n",
    "    if n_components == 2:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if labels is not None:\n",
    "            colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "            for i, label in enumerate(np.unique(labels)):\n",
    "                mask = labels == label\n",
    "                plt.scatter(samples_pca[mask, 0], samples_pca[mask, 1], \n",
    "                          c=colors[i % len(colors)], label=f'Component {label}', alpha=0.7)\n",
    "            plt.legend()\n",
    "        else:\n",
    "            plt.scatter(samples_pca[:, 0], samples_pca[:, 1], alpha=0.7)\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    elif n_components == 3:\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        if labels is not None:\n",
    "            colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "            for i, label in enumerate(np.unique(labels)):\n",
    "                mask = labels == label\n",
    "                ax.scatter(samples_pca[mask, 0], samples_pca[mask, 1], samples_pca[mask, 2],\n",
    "                          c=colors[i % len(colors)], label=f'Component {label}', alpha=0.7)\n",
    "            ax.legend()\n",
    "        else:\n",
    "            ax.scatter(samples_pca[:, 0], samples_pca[:, 1], samples_pca[:, 2], alpha=0.7)\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return samples_pca, pca\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6d1e80e8-43e5-4a94-90c4-1fdf21a88901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### standard interpolants\n",
    "    \n",
    "def beta(t):\n",
    "    return t\n",
    "\n",
    "def beta_dot(t):\n",
    "    return 1.0 \n",
    "\n",
    "#### designed interpolants\n",
    "\n",
    "# dim = 1000\n",
    "# d = dim\n",
    "# M = np.sqrt(d)\n",
    "# def beta(t):\n",
    "#     \"\"\"\n",
    "#     Compute β_t = (1/M) * √(-log(1 + (e^(-M²) - 1)t))\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     t : float or torch.Tensor\n",
    "#         Time parameter\n",
    "#     M : float or torch.Tensor\n",
    "#         Parameter M\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     torch.Tensor\n",
    "#         β_t(t) value\n",
    "#     \"\"\"\n",
    "#     t = torch.as_tensor(t, dtype=torch.float32)\n",
    "#     M_squared = torch.as_tensor(M*M, dtype=torch.float32)\n",
    "    \n",
    "#     # For numerical stability when M² is large\n",
    "#     if M_squared > 20:\n",
    "#         # e^(-M²) ≈ 0, so (e^(-M²) - 1) ≈ -1\n",
    "#         inner = 1 - t\n",
    "#     else:\n",
    "#         exp_neg_M2 = torch.exp(-M_squared)\n",
    "#         inner = 1 + (exp_neg_M2 - 1) * t\n",
    "    \n",
    "#     # Clamp to avoid log(0) or negative values\n",
    "#     inner = torch.clamp(inner, min=1e-10)\n",
    "#     log_term = -torch.log(inner)\n",
    "    \n",
    "#     beta = torch.sqrt(torch.clamp(log_term, min=1e-10)) / M\n",
    "    \n",
    "#     return beta\n",
    "\n",
    "# def beta_dot(t):\n",
    "#     \"\"\"\n",
    "#     Compute dβ_t/dt = -(1/M) * (e^(-M²) - 1) / (2 * √(-log(1 + (e^(-M²) - 1)t)) * (1 + (e^(-M²) - 1)t))\n",
    "    \n",
    "#     Mathematical derivation:\n",
    "#     β_t = (1/M) * √(-log(1 + (e^(-M²) - 1)t))\n",
    "    \n",
    "#     Let u = 1 + (e^(-M²) - 1)t\n",
    "#     Then β_t = (1/M) * √(-log(u))\n",
    "    \n",
    "#     dβ_t/dt = (1/M) * (1/2) * (1/√(-log(u))) * d/dt[-log(u)]\n",
    "#             = (1/M) * (1/2) * (1/√(-log(u))) * (-1/u) * du/dt\n",
    "#             = (1/M) * (1/2) * (1/√(-log(u))) * (-1/u) * (e^(-M²) - 1)\n",
    "#             = -(1/M) * (e^(-M²) - 1) / (2 * √(-log(u)) * u)\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     t : float or torch.Tensor\n",
    "#         Time parameter\n",
    "#     M : float or torch.Tensor\n",
    "#         Parameter M\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     torch.Tensor\n",
    "#         dβ_t/dt value\n",
    "#     \"\"\"\n",
    "#     t = torch.as_tensor(t, dtype=torch.float32)\n",
    "#     M_squared = torch.as_tensor(M*M, dtype=torch.float32)\n",
    "    \n",
    "#     # For numerical stability when M² is large\n",
    "#     if M_squared > 20:\n",
    "#         # e^(-M²) ≈ 0, so (e^(-M²) - 1) ≈ -1\n",
    "#         exp_term = -1.0\n",
    "#         inner = 1 - t\n",
    "#     else:\n",
    "#         exp_neg_M2 = torch.exp(-M_squared)\n",
    "#         exp_term = exp_neg_M2 - 1\n",
    "#         inner = 1 + exp_term * t\n",
    "    \n",
    "#     # Clamp to avoid numerical issues\n",
    "#     inner = torch.clamp(inner, min=1e-10)\n",
    "#     log_term = -torch.log(inner)\n",
    "#     sqrt_log_term = torch.sqrt(torch.clamp(log_term, min=1e-10))\n",
    "    \n",
    "#     # Compute derivative - the negative sign is crucial!\n",
    "#     derivative = -(1/M) * exp_term / (2 * sqrt_log_term * inner)\n",
    "    \n",
    "#     return derivative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ae2dd37c-cced-46e2-9e25-722a9e668ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def drift_b(x, t, r, h):\n",
    "    \"\"\"\n",
    "    Compute b_t(x) = dotβ_t(t) * tanh(h + β_t(t) * ⟨r, x⟩)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : torch.Tensor of shape (num_samples, d)\n",
    "        Input points \n",
    "    r : torch.Tensor of shape (d,)\n",
    "        Fixed vector satisfying |r|_2 = sqrt(d)\n",
    "    beta_t : callable\n",
    "        Function that takes time t and returns β_t(t)\n",
    "    h : float or torch.Tensor\n",
    "        Scalar parameter h in the tanh function\n",
    "    t : float or torch.Tensor\n",
    "        Current time value\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor of shape (num_samples, d)\n",
    "        b_t(x) evaluated at input points x\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate β_t at time t\n",
    "    beta_val = beta(t)\n",
    "    dot_beta = beta_dot(t)\n",
    "    \n",
    "    # Compute inner product ⟨r, x⟩ for each sample\n",
    "    inner_product = torch.sum(x * r, dim=1, keepdim=True)  # (num_samples, 1)\n",
    "    \n",
    "    # Compute β_t * ⟨r, x⟩\n",
    "    beta_inner = beta_val * inner_product  # (num_samples, 1)\n",
    "    \n",
    "    # Compute tanh(h + β_t * ⟨r, x⟩)\n",
    "    tanh_term = torch.tanh(h + beta_inner)  # (num_samples, 1)\n",
    "    \n",
    "    # Compute b_t(x) = dotβ_t * tanh(h + β_t * ⟨r, x⟩) * r\n",
    "    bt_x = dot_beta * tanh_term * r.unsqueeze(0)  # (num_samples, d)\n",
    "    \n",
    "    return bt_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e3035aef-9589-49f2-a745-4dd0c249f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5000\n",
    "dim = 1000\n",
    "r = torch.ones(dim)\n",
    "p = 0.2\n",
    "h = math.log(p/(1-p))/2\n",
    "z0 = torch.randn(num_samples, dim)\n",
    "z1, _ = generate_twomode_GMM(num_samples, dim, p, r, sigma=1.0)\n",
    "\n",
    "D = {'z0': z0, 'z1': z1}\n",
    "\n",
    "t_min_sample = 1e-3\n",
    "t_max_sample = 1-1e-3\n",
    "\n",
    "# t_min_sample = 0\n",
    "# t_max_sample =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bb08ec2f-b648-493c-a817-13bc7103b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RK integration\n",
    "\n",
    "from torchdiffeq import odeint\n",
    "class PFlowRHS(nn.Module):\n",
    "    def __init__(self, drift_b, r, h):\n",
    "        super(PFlowRHS, self).__init__()\n",
    "        self.drift_b = drift_b\n",
    "        self.r = r\n",
    "        self.h = h\n",
    "        \n",
    "    def forward(self, t, states):\n",
    "        (zt,) = states\n",
    "        dzt = self.drift_b(zt, t, self.r, self.h)\n",
    "        return (dzt,)\n",
    "             \n",
    "class PFlowIntegrator:\n",
    "        \n",
    "    def __init__(self):\n",
    "        return        \n",
    "\n",
    "    def __call__(self, drift_b, z0, r, h, T_min, T_max, steps, method='dopri5', return_last = True):\n",
    "\n",
    "        rhs = PFlowRHS(drift_b, r, h)\n",
    "\n",
    "        t = torch.linspace(\n",
    "            T_min, T_max, steps\n",
    "        ).type_as(z0)\n",
    "\n",
    "        int_args = {\n",
    "            'method': method, \n",
    "            # 'atol': c.integration_atol, \n",
    "            # 'rtol': c.integration_rtol,\n",
    "        }\n",
    "\n",
    "        (z,) = odeint(rhs, (z0,), t, **int_args)\n",
    "        if return_last:\n",
    "            return z[-1].clone()\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "pflow = PFlowIntegrator()\n",
    "\n",
    "out = pflow(\n",
    "    drift_b = drift_b,\n",
    "    z0 = D['z0'],\n",
    "    r = r, \n",
    "    h = h, \n",
    "    T_min = t_min_sample,\n",
    "    T_max = t_max_sample,\n",
    "    steps = 5, \n",
    "    method = 'rk4',\n",
    "    return_last = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2d83c7eb-49b9-42ab-9b50-09952ad85063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_gmm_pca(z1, None, n_components=2)\n",
    "# visualize_gmm_pca(out, None, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a51d9ae3-6ab5-4439-af17-27a6394e587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def fit_gmm_density(samples, n_components=2, use_pca=True):\n",
    "    \"\"\"\n",
    "    Fit GMM to samples and return parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : torch.Tensor of shape (num_samples, dim)\n",
    "        Input samples\n",
    "    n_components : int, default=2\n",
    "        Number of GMM components\n",
    "    use_pca : bool, default=True\n",
    "        Apply PCA if dim > 2\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights : np.ndarray\n",
    "        Component weights\n",
    "    means : np.ndarray\n",
    "        Component means\n",
    "    covariances : np.ndarray\n",
    "        Component covariances\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to numpy\n",
    "    if torch.is_tensor(samples):\n",
    "        samples = samples.detach().cpu().numpy()\n",
    "    \n",
    "    # Apply PCA if high-dimensional\n",
    "    if use_pca and samples.shape[1] > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        samples = pca.fit_transform(samples)\n",
    "        print(f\"PCA: {pca.explained_variance_ratio_.sum():.1%} variance explained\")\n",
    "    \n",
    "    # Fit GMM\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    gmm.fit(samples)\n",
    "    \n",
    "    # Visualize density\n",
    "    # x_min, x_max = samples[:, 0].min() - 1, samples[:, 0].max() + 1\n",
    "    # y_min, y_max = samples[:, 1].min() - 1, samples[:, 1].max() + 1\n",
    "    # xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "    #                      np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # log_prob = gmm.score_samples(grid_points)\n",
    "    # prob = np.exp(log_prob).reshape(xx.shape)\n",
    "    \n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # plt.contourf(xx, yy, prob, levels=15, alpha=0.6, cmap='viridis')\n",
    "    # plt.scatter(samples[:, 0], samples[:, 1], alpha=0.7, s=20, c='white', edgecolors='black')\n",
    "    # plt.colorbar(label='Density')\n",
    "    # plt.title('GMM Density Estimation')\n",
    "    # plt.show()\n",
    "    \n",
    "    # Print parameters\n",
    "    print(f\"Weights: {gmm.weights_}\")\n",
    "    print(f\"Means:\\n{gmm.means_}\")\n",
    "    print(f\"Covariances:\\n{gmm.covariances_}\")\n",
    "    \n",
    "    return gmm.weights_, gmm.means_, gmm.covariances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7d8fc070-946a-458f-8d39-c64b9726489a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth\n",
      "Weights: [0.8024 0.1976]\n",
      "Means:\n",
      "[[-12.49770977]\n",
      " [ 50.74980932]]\n",
      "Covariances:\n",
      "[[[1.02485029]]\n",
      "\n",
      " [[0.97934768]]]\n",
      "---------------------------------\n",
      "generation\n",
      "Weights: [0.9442 0.0558]\n",
      "Means:\n",
      "[[-3.39617267]\n",
      " [57.46713684]]\n",
      "Covariances:\n",
      "[[[1.06909726]]\n",
      "\n",
      " [[1.30990296]]]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=1)\n",
    "z1_pca = pca.fit_transform(z1)\n",
    "out_pca = pca.fit_transform(out)\n",
    "\n",
    "print('truth')\n",
    "weights, means, covariances = fit_gmm_density(z1_pca, n_components=2, use_pca=False)\n",
    "print('---------------------------------')\n",
    "print('generation')\n",
    "weights, means, covariances = fit_gmm_density(out_pca, n_components=2, use_pca=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dadf3d5-adde-47a5-b4be-f92168e51183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8289bc6-d7ce-4d64-a4fa-273cf8a4223e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df7c66-5818-47ef-96e4-1ec4845c5416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MYKERNEL",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
